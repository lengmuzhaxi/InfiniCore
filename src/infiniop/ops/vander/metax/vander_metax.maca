#include "vander_metax.h"
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_handle.h"
#include <mcr/mc_runtime.h>
#include <cstdint>
#include <algorithm>
#include <cmath>
#include <cstdio>
#include <maca_fp16.h>
#include <maca_bfloat16.h>
using nv_bfloat16 = __maca_bfloat16;
using nv_bfloat162 = __maca_bfloat162;

namespace op::vander::metax {
template <typename T>
__global__ void vander_kernel(
    T * __restrict__ output,        // [rows, cols]
    const T * __restrict__ input,   // [rows]
    size_t rows,
    size_t cols,
    bool increasing) {
    
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t total_elements = rows * cols;

    if (idx < total_elements) {
        size_t row = idx / cols;
        size_t col = idx % cols;

        // 加载输入 (同一个 row 的不同 col 线程会读取同一个 input[row]，L1 Cache 友好)
        float x = static_cast<float>(input[row]);
        
        // 计算指数
        // increasing=True:  x^0, x^1, ..., x^(N-1)
        // increasing=False: x^(N-1), ..., x^1, x^0
        float power = increasing ? static_cast<float>(col) 
                                 : static_cast<float>(cols - 1 - col);
        
        float res = powf(x, power);

        output[idx] = static_cast<T>(res);
    }
}

// ==================================================================
// 3. Kernel Launch Logic
// ==================================================================
template <typename T>
void launch_kernel(
    void *output, 
    const void *input, 
    const VanderInfo& info,
    void *stream) {

    // 1. 准备指针
    auto in_ptr = reinterpret_cast<const T *>(input);
    auto out_ptr = reinterpret_cast<T *>(output);
    auto mc_stream = reinterpret_cast<mcStream_t>(stream);
    
    // 2. 准备参数
    size_t rows = info.rows();
    size_t cols = info.cols();
    bool increasing = info.increasing();
    
    // 计算 Grid Size
    size_t total_elements = rows * cols;
    size_t block_size = 256;
    size_t grid_size = (total_elements + block_size - 1) / block_size;
    
    // 3. 启动 Kernel
    vander_kernel<T>
        <<<grid_size, block_size, 0, mc_stream>>>(
            out_ptr, in_ptr, rows, cols, increasing
        );
}

// ==================================================================
// 4. Descriptor 实现
// ==================================================================
struct Descriptor::Opaque {};

Descriptor::~Descriptor() { 
    if (_opaque) delete _opaque; 
}

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_, 
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t out_desc, 
    infiniopTensorDescriptor_t input_desc, 
    int N, 
    int increasing) {

    auto handle = reinterpret_cast<device::metax::Handle *>(handle_);

    // 1. 创建并校验 Info
    auto info_result = VanderInfo::create(out_desc, input_desc, N, increasing);
    if (!info_result) return info_result.status();

    // 2. 创建 Descriptor
    size_t workspace_size = 0;

    *desc_ptr = new Descriptor(
        new Opaque(), 
        info_result.take(), 
        workspace_size, 
        handle->device, 
        handle->device_id
    );
    
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace, 
    size_t workspace_size, 
    void *output,
    const void *input, 
    void *stream) const {

    auto dtype = _info.dtype();

    // 3. 根据数据类型分发 Kernel
    switch (dtype) {
    case INFINI_DTYPE_F16:
        launch_kernel<__half>(output, input, _info, stream);
        break;
    case INFINI_DTYPE_BF16:
        launch_kernel<__maca_bfloat16>(output, input, _info, stream);
        break;
    case INFINI_DTYPE_F32:
        launch_kernel<float>(output, input, _info, stream);
        break;
    case INFINI_DTYPE_F64:
        launch_kernel<double>(output, input, _info, stream);
        break;
    default:
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }

    return INFINI_STATUS_SUCCESS;
}

} // namespace op::vander::metax