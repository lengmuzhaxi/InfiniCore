#include "log_softmax_metax.h"
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_handle.h"
#include <mcr/mc_runtime.h>
#include <maca_fp16.h>
#include <maca_bfloat16.h>
#include <cmath>
#include <limits>
#include <cstdint>
#include <algorithm>

namespace op::log_softmax::metax {

// ==================================================================
// Device Helpers: 类型转换与归约
// ==================================================================

__device__ __forceinline__ float to_float(float val) { return val; }
__device__ __forceinline__ float to_float(double val) { return static_cast<float>(val); }
__device__ __forceinline__ float to_float(__half val) { return __half2float(val); }
__device__ __forceinline__ float to_float(__maca_bfloat16 val) { return __bfloat162float(val); }

// ==================================================================
// Warp Reduction Helpers
// ==================================================================
template <typename T>
__device__ __forceinline__ T warp_reduce_max(T val) {
    for (int offset = 32 / 2; offset > 0; offset /= 2) {
        T shuffled = __shfl_down_sync(0xffffffff, val, offset);
        val = (val > shuffled) ? val : shuffled;
    }
    return val;
}

template <typename T>
__device__ __forceinline__ T warp_reduce_sum(T val) {
    for (int offset = 32 / 2; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}

// ==================================================================
// Block Reduction Helpers
// ==================================================================
template <typename T>
__device__ __forceinline__ T block_reduce_max(T val) {
    static __shared__ float shared[32]; // Max 32 warps per block
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    val = warp_reduce_max(val);

    if (lane == 0) shared[wid] = val;
    __syncthreads();

    // 假设 BlockDim.x 不超过 1024 (32 warps)
    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : -INFINITY;
    
    if (wid == 0) val = warp_reduce_max(val);
    
    return val;
}

template <typename T>
__device__ __forceinline__ T block_reduce_sum(T val) {
    static __shared__ float shared[32];
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    val = warp_reduce_sum(val);

    if (lane == 0) shared[wid] = val;
    __syncthreads();

    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;
    
    if (wid == 0) val = warp_reduce_sum(val);
    
    return val;
}

// ==================================================================
// Kernel: LogSoftmax (Online Softmax / 3-Pass Algorithm)
// ==================================================================
template <typename T>
__global__ void log_softmax_kernel(
    T * __restrict__ output,        // [Outer, Dim, Inner]
    const T * __restrict__ input,   // [Outer, Dim, Inner]
    size_t dim_size,
    size_t inner_size
) {
    // 共享内存用于存储 Block Reduction 的结果广播
    __shared__ float s_max;
    __shared__ float s_sum;

    unsigned int tid = threadIdx.x;
    unsigned int bid = blockIdx.x;

    // 1. 计算当前 Slice 的基地址
    // GridDim.x = Outer * Inner
    size_t outer_idx = bid / inner_size;
    size_t inner_idx = bid % inner_size;

    // Layout: [outer, dim, inner]
    // Base offset = outer * (dim_size * inner_size) + inner_idx
    size_t base_offset = outer_idx * dim_size * inner_size + inner_idx;
    size_t stride = inner_size; // 元素在 Dim 维度的跨度

    // ============================================================
    // Pass 1: Find Max (为了数值稳定性)
    // ============================================================
    float local_max = -INFINITY;
    for (size_t i = tid; i < dim_size; i += blockDim.x) {
        float val = to_float(input[base_offset + i * stride]);
        if (val > local_max) {
            local_max = val;
        }
    }
    
    // Block Reduction 得到全局 Max
    float global_max = block_reduce_max(local_max);
    if (tid == 0) s_max = global_max;
    __syncthreads();
    global_max = s_max; // 广播

    // ============================================================
    // Pass 2: Calculate Sum of Exponentials
    // sum(exp(x - max))
    // ============================================================
    float local_sum = 0.0f;
    for (size_t i = tid; i < dim_size; i += blockDim.x) {
        float val = to_float(input[base_offset + i * stride]);
        local_sum += expf(val - global_max);
    }

    // Block Reduction 得到全局 Sum
    float global_sum = block_reduce_sum(local_sum);
    if (tid == 0) s_sum = global_sum;
    __syncthreads();
    global_sum = s_sum; // 广播

    // 计算 LogSumExp: log(sum) + max
    float log_sum_exp = logf(global_sum) + global_max;

    // ============================================================
    // Pass 3: Calculate Final Output
    // output = x - LogSumExp
    // ============================================================
    for (size_t i = tid; i < dim_size; i += blockDim.x) {
        size_t idx = base_offset + i * stride;
        float val = to_float(input[idx]);
        output[idx] = static_cast<T>(val - log_sum_exp);
    }
}

// ==================================================================
// Host Implementation
// ==================================================================

struct Descriptor::Opaque {};

template <typename T>
void launch_kernel(
    void *output, 
    const void *input, 
    const LogSoftmaxInfo& info,
    void *stream) {

    // 1. 准备指针
    auto in_ptr = reinterpret_cast<const T *>(input);
    auto out_ptr = reinterpret_cast<T *>(output);
    
    auto mc_stream = reinterpret_cast<mcStream_t>(stream);
    
    // 2. 准备形状参数
    size_t dim_size = info.dim_size();
    size_t outer_size = info.outer_size();
    size_t inner_size = info.inner_size();

    // 3. 计算 Grid/Block
    // Grid: 总切片数 (Outer * Inner)
    // 每个 Block 处理 1 个 Slice (Dim 维度)
    size_t total_slices = outer_size * inner_size;
    
    // Block: 选择一个合理的 Block Size (例如 256)
    unsigned int threads_per_block = 256;
    
    // 根据 dim_size 调整 block size
    if (dim_size < 256) {
        threads_per_block = 128;
    }
    if (dim_size < 128) {
        threads_per_block = 64;
    }
    if (dim_size < 64) {
        threads_per_block = 32;
    }

    // 4. 启动 Kernel
    log_softmax_kernel<T>
        <<<total_slices, threads_per_block, 0, mc_stream>>>(
            out_ptr, 
            in_ptr, 
            dim_size, 
            inner_size
        );
}

Descriptor::~Descriptor() { 
    if (_opaque) delete _opaque; 
}

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_, Descriptor **desc_ptr,
    infiniopTensorDescriptor_t output_desc, 
    infiniopTensorDescriptor_t input_desc, 
    int dim) {

    auto handle = reinterpret_cast<device::metax::Handle *>(handle_);

    auto info_result = LogSoftmaxInfo::create(output_desc, input_desc, dim);
    if (!info_result) return info_result.status();

    // LogSoftmax 此实现为 Online 算法，不需要额外的 Workspace
    size_t workspace_size = 0;

    *desc_ptr = new Descriptor(new Opaque(), info_result.take(), workspace_size, handle->device, handle->device_id);
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace, 
    size_t workspace_size, 
    void *output, 
    const void *input, 
    void *stream) const {

    auto dtype = _info.dtype();

    switch (dtype) {
    case INFINI_DTYPE_F16:
        launch_kernel<__half>(output, input, _info, stream);
        break;
    case INFINI_DTYPE_BF16:
        launch_kernel<__maca_bfloat16>(output, input, _info, stream);
        break;
    case INFINI_DTYPE_F32:
        launch_kernel<float>(output, input, _info, stream);
        break;
    case INFINI_DTYPE_F64:
        launch_kernel<double>(output, input, _info, stream);
        break;
    default:
        // LogSoftmax 不支持整型输入
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }

    return INFINI_STATUS_SUCCESS;
}

} // namespace op::log_softmax::metax