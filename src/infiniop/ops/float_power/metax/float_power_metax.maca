#include "float_power_metax.h" 
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_handle.h"
#include <mcr/mc_runtime.h>
#include <cstdint>
#include <algorithm>
#include <cmath>
#include <maca_fp16.h>
#include <maca_bfloat16.h>
using nv_bfloat16 = __maca_bfloat16;
using nv_bfloat162 = __maca_bfloat162;


namespace op::float_power::metax {

// ==================================================================
// 2. Kernel 定义 (对应原 .cuh 内容)
// ==================================================================

// 基础定义: 向量化数据打包结构
template <typename T, int N>
struct alignas(sizeof(T) * N) Pack {
    T val[N];
};

// Functor: 仅负责核心数学计算逻辑
struct FloatPowerFunctor {
    template <typename T_IN>
    __device__ __forceinline__ float compute(const T_IN &input, float exponent_val) const {
        // 将输入转为 float 参与计算
        float in_f = static_cast<float>(input);
        return powf(in_f, exponent_val);
    }
};

// Kernel 1: 通用处理 (Grid-Stride Loop)
template <typename T_OUT, typename T_IN, typename T_EXP>
__global__ void float_power_kernel(
    T_OUT * __restrict__ output, 
    const T_IN * __restrict__ input, 
    const T_EXP * __restrict__ exponent, 
    float scalar_exponent, 
    bool is_scalar, 
    size_t numel, 
    FloatPowerFunctor functor) {
    
    for (size_t idx = blockIdx.x * blockDim.x + threadIdx.x; 
         idx < numel; 
         idx += blockDim.x * gridDim.x) {
         
        float exp_val_f = is_scalar ? scalar_exponent : static_cast<float>(exponent[idx]);
        output[idx] = static_cast<T_OUT>(functor.compute(input[idx], exp_val_f));
    }
}

// Kernel 2: 标量模式向量化 Kernel
template <typename T_OUT, typename T_IN, int PackSize>
__global__ void float_power_kernel_vectorized_scalar(
    T_OUT * __restrict__ output, 
    const T_IN * __restrict__ input, 
    float scalar_exponent, 
    size_t num_packs, 
    FloatPowerFunctor functor) {
    
    using PackTypeIn = Pack<T_IN, PackSize>;
    using PackTypeOut = Pack<T_OUT, PackSize>;

    auto in_vec = reinterpret_cast<const PackTypeIn*>(input);
    auto out_vec = reinterpret_cast<PackTypeOut*>(output);
    
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < num_packs) {
        PackTypeIn in_pack = in_vec[idx];
        PackTypeOut out_pack;
        
        #pragma unroll
        for (int i = 0; i < PackSize; ++i) {
            out_pack.val[i] = static_cast<T_OUT>(functor.compute(in_pack.val[i], scalar_exponent));
        }
        out_vec[idx] = out_pack;
    }
}

// Kernel 3: 张量模式向量化 Kernel
template <typename T_OUT, typename T_IN, int PackSize>
__global__ void float_power_kernel_vectorized_tensor(
    T_OUT * __restrict__ output, 
    const T_IN * __restrict__ input, 
    const T_IN * __restrict__ exponent, 
    size_t num_packs, 
    FloatPowerFunctor functor) {
    
    using PackTypeIn = Pack<T_IN, PackSize>;
    using PackTypeOut = Pack<T_OUT, PackSize>;

    auto in_vec = reinterpret_cast<const PackTypeIn*>(input);
    auto exp_vec = reinterpret_cast<const PackTypeIn*>(exponent);
    auto out_vec = reinterpret_cast<PackTypeOut*>(output);
    
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < num_packs) {
        PackTypeIn in_pack = in_vec[idx];
        PackTypeIn exp_pack = exp_vec[idx]; 
        PackTypeOut out_pack;
        
        #pragma unroll
        for (int i = 0; i < PackSize; ++i) {
            float e = static_cast<float>(exp_pack.val[i]);
            out_pack.val[i] = static_cast<T_OUT>(functor.compute(in_pack.val[i], e));
        }
        out_vec[idx] = out_pack;
    }
}

// ==================================================================
// 3. 辅助函数与 Launcher
// ==================================================================

// 辅助函数: 检查内存地址对齐情况
template <typename T>
static inline bool is_aligned(const void *ptr, size_t alignment) {
    return reinterpret_cast<uintptr_t>(ptr) % alignment == 0;
}

// Launcher Implementation
template <typename T_OUT, typename T_IN>
void launch_kernel(
    void *output, 
    const void *input, 
    const void *exponent, 
    const FloatPowerInfo &info,
    void *stream) {

    size_t numel = info.num_elements();
    bool is_scalar = info.is_scalar_exponent();
    float scalar_exp = info.scalar_exponent();

    auto out_ptr = reinterpret_cast<T_OUT *>(output);
    auto in_ptr = reinterpret_cast<const T_IN *>(input);
    // 假设指数 Tensor 的数据类型与输入 Tensor 一致
    auto exp_ptr = reinterpret_cast<const T_IN *>(exponent);

    auto mc_stream = reinterpret_cast<mcStream_t>(stream);
    FloatPowerFunctor functor;

    // ------------------------------------------------------------------
    // 向量化分发路径
    // ------------------------------------------------------------------
    constexpr int AlignBytes = 16; 
    constexpr int PackSizeIn = AlignBytes / sizeof(T_IN);

    // 检查输入输出类型大小是否一致
    bool types_same_size = (sizeof(T_IN) == sizeof(T_OUT));

    bool can_vectorize_base = types_same_size &&
                              (PackSizeIn > 1) &&
                              (numel % PackSizeIn == 0) &&
                              is_aligned<T_IN>(input, AlignBytes) &&
                              is_aligned<T_OUT>(output, AlignBytes);

    if (can_vectorize_base) {
        size_t num_packs = numel / PackSizeIn;
        size_t block_size = 256;
        size_t grid_size = (num_packs + block_size - 1) / block_size;

        if (is_scalar) {
            // 路径 A1: 标量指数向量化
            float_power_kernel_vectorized_scalar<T_OUT, T_IN, PackSizeIn>
                <<<grid_size, block_size, 0, mc_stream>>>(
                    out_ptr, in_ptr, scalar_exp, num_packs, functor
                );
            return;
        } else if (is_aligned<T_IN>(exponent, AlignBytes)) {
            // 路径 A2: 张量指数向量化
            float_power_kernel_vectorized_tensor<T_OUT, T_IN, PackSizeIn>
                <<<grid_size, block_size, 0, mc_stream>>>(
                    out_ptr, in_ptr, exp_ptr, num_packs, functor
                );
            return;
        }
    }

    // ------------------------------------------------------------------
    // 通用回退路径
    // ------------------------------------------------------------------
    size_t block_size = 256;
    size_t grid_size = (numel + block_size - 1) / block_size;

    float_power_kernel<T_OUT, T_IN, T_IN>
        <<<grid_size, block_size, 0, mc_stream>>>(
            out_ptr, in_ptr, exp_ptr, scalar_exp, is_scalar, numel, functor
        );
}

// ==================================================================
// 4. Descriptor 接口实现
// ==================================================================
struct Descriptor::Opaque {};

Descriptor::~Descriptor() { if (_opaque) delete _opaque; }

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_, Descriptor **desc_ptr,
    infiniopTensorDescriptor_t y, 
    infiniopTensorDescriptor_t x,
    infiniopTensorDescriptor_t exponent, 
    float scalar_exponent) {

    auto handle = reinterpret_cast<device::metax::Handle *>(handle_);
    auto info_result = FloatPowerInfo::create(y, x, exponent, scalar_exponent);
    if (!info_result) return info_result.status();

    size_t workspace_size = 0;
    *desc_ptr = new Descriptor(new Opaque(), info_result.take(), workspace_size, handle->device, handle->device_id);
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace, size_t workspace_size, void *output,
    const void *input, const void *exponent, 
    void *stream) const {

    auto in_dtype = _info.input_dtype();
    auto out_dtype = _info.output_dtype();

    // ==================================================================
    // 显式双重分发 (注意: half 和 nv_bfloat16 已在上方适配)
    // ==================================================================

    switch (in_dtype) {

    case INFINI_DTYPE_F32:
        switch (out_dtype) {
        case INFINI_DTYPE_F32:
            launch_kernel<float, float>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_F64:
            launch_kernel<double, float>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_F16:
            launch_kernel<__half, float>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_BF16:
            launch_kernel<nv_bfloat16, float>(output, input, exponent, _info, stream);
            break;
        default: return INFINI_STATUS_BAD_TENSOR_DTYPE;
        }
        break;

    case INFINI_DTYPE_F64:
        switch (out_dtype) {
        case INFINI_DTYPE_F32:
            launch_kernel<float, double>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_F64:
            launch_kernel<double, double>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_F16:
            launch_kernel<__half, double>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_BF16:
            launch_kernel<nv_bfloat16, double>(output, input, exponent, _info, stream);
            break;
        default: return INFINI_STATUS_BAD_TENSOR_DTYPE;
        }
        break;

    case INFINI_DTYPE_F16:
        switch (out_dtype) {
        case INFINI_DTYPE_F32:
            launch_kernel<float, __half>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_F64:
            launch_kernel<double, __half>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_F16:
            launch_kernel<__half, __half>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_BF16:
            launch_kernel<nv_bfloat16, __half>(output, input, exponent, _info, stream);
            break;
        default: return INFINI_STATUS_BAD_TENSOR_DTYPE;
        }
        break;

    case INFINI_DTYPE_BF16:
        switch (out_dtype) {
        case INFINI_DTYPE_F32:
            launch_kernel<float, nv_bfloat16>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_F64:
            launch_kernel<double, nv_bfloat16>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_F16:
            launch_kernel<__half, nv_bfloat16>(output, input, exponent, _info, stream);
            break;
        case INFINI_DTYPE_BF16:
            launch_kernel<nv_bfloat16, nv_bfloat16>(output, input, exponent, _info, stream);
            break;
        default: return INFINI_STATUS_BAD_TENSOR_DTYPE;
        }
        break;

    default:
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }

    return INFINI_STATUS_SUCCESS;
}

} // namespace op::float_power::metax